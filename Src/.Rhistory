all_test$GarageArea[is.na(all_test$GarageArea)] <- 'No Garage'
# Add new levels and change NA into 'None' level, meaning No Garage
all_test$GarageQual <- addLevel(all_test$GarageQual, "None")
all_test$GarageQual <- ifelse(is.na(all_test$GarageQual), "None", paste(all_test$GarageQual))
all_test$GarageQual <- as.factor(all_test$GarageQual)
# Add new levels and change NA into 'None' level, meaning No Garage
all_test$GarageCond <- addLevel(all_test$GarageCond, "None")
all_test$GarageCond <- ifelse(is.na(all_test$GarageCond), "None", paste(all_test$GarageCond))
all_test$GarageCond <- as.factor(all_test$GarageCond)
# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtQual <- addLevel(all_test$BsmtQual, "None")
all_test$BsmtQual <- ifelse(is.na(all_test$BsmtQual), "None", paste(all_test$BsmtQual))
all_test$BsmtQual <- as.factor(all_test$BsmtQual)
# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtExposure <- addLevel(all_test$BsmtExposure, "None")
all_test$BsmtExposure <- ifelse(is.na(all_test$BsmtExposure), "None", paste(all_test$BsmtExposure))
all_test$BsmtExposure <- as.factor(all_test$BsmtExposure)
# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtFinType1 <- addLevel(all_test$BsmtFinType1, "None")
all_test$BsmtFinType1 <- ifelse(is.na(all_test$BsmtFinType1), "None", paste(all_test$BsmtFinType1))
all_test$BsmtFinType1 <- as.factor(all_test$BsmtFinType1)
# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtCond <- addLevel(all_test$BsmtCond, "None")
all_test$BsmtCond <- ifelse(is.na(all_test$BsmtCond), "None", paste(all_test$BsmtCond))
all_test$BsmtCond <- as.factor(all_test$BsmtCond)
# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtFinType2 <- addLevel(all_test$BsmtFinType2, "None")
all_test$BsmtFinType2 <- ifelse(is.na(all_test$BsmtFinType2), "None", paste(all_test$BsmtFinType2))
all_test$BsmtFinType2 <- as.factor(all_test$BsmtFinType2)
# Set Basement Area equals 0 for houses which do not have basement
all_test$BsmtFinSF1[is.na(all_test$BsmtFinSF1)] <-0
all_test$BsmtFinSF2[is.na(all_test$BsmtFinSF2)] <-0
all_test$BsmtUnfSF[is.na(all_test$BsmtUnfSF)] <-0
# Set Basement Area equals 0 for houses which has zero linear feet of street connected to property
all_test$LotFrontage[is.na(all_test$LotFrontage)] <-0
drop.na.columns <- c( "MasVnrType","LotFrontage")
all_test<- all_test[ , !(names(all_test) %in% drop.na.columns)]
all_test <- na.omit(all_test)
all <- all_test
# factorize some of the variables
all$Foundation <- as.factor(all$Foundation)
all$Heating <- as.factor(all$Heating)
all$RoofStyle <- as.factor(all$RoofStyle)
all$RoofMatl <- as.factor(all$RoofMatl)
all$LandContour <- as.factor(all$LandContour)
all$BldgType <- as.factor(all$BldgType)
all$HouseStyle <- as.factor(all$HouseStyle)
all$Condition1 <- as.factor(all$Condition1)
all$Condition2 <- as.factor(all$Condition2)
all$GarageArea <- as.numeric(all$GarageArea)
na_count_test <-sapply(all, function(y) sum(length(which(is.na(y)))))
na_count_test <- data.frame(na_count_test)
write_excel_csv(all, "New_Data.csv")
#Load treated and processed data 'New_Data.csv'
New_Data <- read_csv("New_Data.csv")
#Load the data with distance between ISU and
#distance <- read_csv("distance.csv")
distance <- read_csv("/Users/lizhizicui/Desktop/Final-Project---Applied-Data-Science/Data/Processed/distance.csv")
#Add the distance data to both Train and Test datasets.
d_train <- left_join(New_Data, distance, by = "Neighborhood")
#d_train1 <- d_train[,-c(73, 75, 7, 74, 57,4, 13, 83,84)]
d_train_noNAs <- na.omit(d_train)
df <- data.frame(tapply(d_train_noNAs$SalePrice, d_train_noNAs$Neighborhood, mean))
library(data.table)
library(forcats)
neighb <- setDT(df, keep.rownames = TRUE)[]
colnames(neighb)[2] <- "mean.price"
#Order values in ascending order.
neighb <- neighb[order(mean.price), ]
neighb$rn <- factor(neighb$rn, levels=unique(as.character(neighb$rn)) )
#Create categories for mean sales price of the neighborhood.
d_train_noNAs$Mean.price <- cut(d_train_noNAs$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )
#plot the results
ggplot(neighb) + geom_col(aes(neighb$rn, y = neighb$mean.price,  fill = df$rn)) +
coord_flip() +
ggtitle("Average Sales Price by Neighborhood") +
ylab("Average Sales Price") + scale_y_continuous(labels = scales::dollar) +
xlab("Neighborhood") + coord_flip()
ggplot(data=all, aes(x=(Neighborhood))) +
geom_histogram(stat='count')+
geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Distribution of houses count by neighborhood")
#Distribution of houses per neighborhood category
pander(table(d_train_noNAs$Neighborhood, d_train_noNAs$Mean.price))
summary(all$SalePrice)
hist(all$SalePrice,
main = 'Histogram of Sale Price',
xlab = 'Sale Price',
breaks=seq(30000, 760000, 5000),prob=TRUE)
lines(density(all$SalePrice), col="blue", lwd=2)
lines(density(all$SalePrice, adjust=2), lty="dotted")
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(OverallQual), y=SalePrice))+
geom_boxplot() + labs(x='Overall Quality') +
scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma) +
ggtitle("Boxplot - Sales Price and Overall Quality")
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(OverallCond), y=SalePrice))+
geom_boxplot() + labs(x='Overall Condition')+
scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma) +
ggtitle("Boxplot - Sales Price and Overall Condition")
ggplot(data=all[!is.na(all$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1)) +
scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma) +
ggtitle("Sales Price and Ground Living Area")+
labs(x = 'Ground Living Area (in square feet)')
# potential outliers found
outlier1 <- all%>% filter(GrLivArea > 4500)
outlier1$Id
all[c(524, 1299, 2550), c('SalePrice', 'GrLivArea', 'OverallQual')]
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(GarageCars), y=SalePrice))+
geom_boxplot() + labs(x='Size of Garage/car') +
scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma)+
ggtitle("Sales Price and Garage Capacity")
#potential outliers found
outlier2 <- all %>%
filter(GarageCars == 3)%>%
filter(SalePrice > 700000)
outlier2$Id
#check the other dimention information of outliers
all[c(524, 1299, 692, 1183), c('SalePrice', 'GrLivArea', 'OverallQual','GarageCars')]
ggplot(data=all[!is.na(all$SalePrice),], aes(x=BsmtFinSF1, y=SalePrice))+
geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
scale_x_continuous(limits = c(100 ,2500)) +
ggtitle("Sales Price and Basement Area")+
labs(x = 'Basement Area (in square feet)')
ggplot(data=all[!is.na(all$SalePrice),], aes(x=YearBuilt, y=SalePrice))+
geom_point(col='black') +
geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
scale_x_continuous(limits = c(1860 ,2020))+
ggtitle("Sales Price and Year of house built")+
labs(x = 'Year of house built')
#OLS Model 1 - very naive considering only the variab
# naive.model <- lm(SalePrice ~ Car.miles + OverallQual + OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model)
print("Model 1.0: Car.Time.min")
naive.model0 <- lm(SalePrice ~ Car.Time.min + OverallQual +OverallCond + GrLivArea, data = d_train_noNAs)
summary(naive.model0)
# naive.model1 <- lm(SalePrice ~ Walk.miles + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model1)
naive.model1.1 <- lm(SalePrice ~ Walk.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model1.1)
naive.model2 <- lm(SalePrice ~ Bus.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model2)
m2.train <- lm(SalePrice ~ Bus.time.min + ., data = d_train_noNAs)
#Show only relevant coefficients
a2Pval <- summary(m2.train)$coefficients[1:2, 1:4]
a2Pval
set.seed(12345) # = Seed for replication
#d_train_noNAs <- na.omit(d_train)
x <- model.matrix(SalePrice ~ ., data = d_train_noNAs)[ ,-79]
y <- d_train_noNAs$SalePrice
### We then fit a Lasso regression model (alpha = 1)
fit.lasso <- glmnet(x, y, alpha = 1, family = "gaussian")
plot(fit.lasso, xvar = "lambda", label = TRUE)
### Now we cross-validate
cv.lasso <- cv.glmnet(x, y)
#cv.lasso$lambda.min
#cv.lasso$lambda.1se
#coef(cv.lasso, s = "lambda.min")
plot(cv.lasso)
title(sub = "Graph 3: Cross Validation for Lasso", cex = 1.5)
set.seed(12345) # = Seed for replication
### Extract coefficients corresponding to lambda.min (minimum mean cross-validated error)
myCoefs <- coef(cv.lasso, s = "lambda.1se")
# print(as.matrix(min.coef))
myLasso.Results <- data.frame(
features = myCoefs@Dimnames[[1]][ which(myCoefs != 0 ) ], #intercept included
coefs    = myCoefs              [ which(myCoefs != 0 ) ]  #intercept included
)
# myLasso.Results$level.coefs <- cut(myLasso.Results$coefs, 5, labels = c("lowest","low", "med", "high", "highest"))
#
# table(myLasso.Results$level.coefs)
#
# summary(myLasso.Results$coefs)
#
# ggplot(myLasso.Results) + geom_bar(aes(myLasso.Results$level.coefs, fill = myLasso.Results$level.coefs)) + coord_flip()
pander(myLasso.Results)
#pander(filter(myLasso.Results, level.coefs != "low" ))
myLasso.Results <- myLasso.Results[-1,]
var.lasso <- myLasso.Results[,1]
var.lasso
m4.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = d_train_noNAs)
summary(m4.train)
pref.data <- d_train_noNAs %>%
filter(SalePrice >= 250000)
#& d_train_noNAs$SalePrice > 150000
m5.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = pref.data)
summary(m5.train)
# Neighborhood
train <- read.csv("New_Data.csv")
train <- na.omit(train)
dropVars <- c('YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotalRmsAbvGrd')
train <- train[,!(names(train) %in% dropVars)]
train <- train[-c(524, 1299, 2550),]
write_csv(train, 'Final_data.csv')
rain <- read.csv('Final_data.csv')
set.seed(123)
quick_RF <- randomForest(x = train[, -76], y = train$SalePrice, ntree = 200,importance = TRUE)
dropVars <- c('YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotalRmsAbvGrd')
train <- train[,!(names(train) %in% dropVars)]
train <- train[-c(524, 1299, 2550),]
write_csv(train, 'Final_data.csv')
train <- read.csv('Final_data.csv')
set.seed(123)
quick_RF <- randomForest(x = train[, -76], y = train$SalePrice, ntree = 200,importance = TRUE)
train$Mean.price <- cut(train$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )
a <- train%>%
filter(Mean.price == 'Higher')
train1 <- train[1:1092,]
test1 <- train[1092:1447,]
# step function
train1 <- subset(train1, select = -c(Utilities,Street))
test1 <- subset(test1, select = -c(Utilities,Street))
test1 <- test1[!test1$Id == 1004,]
test1 <- test1[!test1$Id == 1299,]
test1 <- test1[!test1$Id == 1001,]
test1 <- test1[!test1$Id == 1188,]
test1 <- test1[!test1$Id == 411,]
test1 <- test1[!test1$Id == 251,]
test1 <- test1[!test1$Id == 596,]
# Neighborhood
train <- read.csv("New_Data.csv")
train <- na.omit(train)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use = "pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
#select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use = "pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
#select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use = "pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
#select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use = "pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
#select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
dropVars <- c('YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotalRmsAbvGrd')
train <- train[,!(names(train) %in% dropVars)]
train <- train[-c(524, 1299, 2550),]
write_csv(train, 'Final_data.csv')
train <- read.csv('Final_data.csv')
set.seed(123)
quick_RF <- randomForest(x = train[, -76], y = train$SalePrice, ntree = 200,importance = TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
ggplot(imp_DF[1:20,], aes(x = reorder(Variables, MSE), y = MSE, fill = MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position = "none")
library(MASS)
LDA <- lda(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea,data = train1)
#  make predictions and summarize accuracy - training set
y_hat_LDA <- predict(LDA, newdata = train1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA1 <- table(train1$Mean.price, z_LDA)
tbl_LDA1
sum(diag(tbl_LDA1))/sum(tbl_LDA)
n = sum(rf_cm) # number of instances
rf_cm = table(test1[,75], y_pred_test)
RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred_train = predict(RF, newdata = train1[-c(74,75)])
y_pred_test = predict(RF, newdata = test1[-c(74,75)])
# make predictions and summarize accuracy - training set
rf_cm1 = table(train1[,75], y_pred_train)
rf_cm1
sum(diag(rf_cm1))/sum(rf_cm1)
# make predictions and summarize accuracy - testing set
rf_cm = table(test1[,75], y_pred_test)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
plot(RF)
n = sum(rf_cm) # number of instances
nc = nrow(rf_cm) # number of classes
diag = diag(rf_cm) # number of correctly classified instances per class
rowsums = apply(rf_cm, 1, sum) # number of instances per class
colsums = apply(rf_cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
n = sum(rf_cm) # number of instances
nc = nrow(rf_cm) # number of classes
diag = diag(rf_cm) # number of correctly classified instances per class
rowsums = apply(rf_cm, 1, sum) # number of instances per class
colsums = apply(rf_cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred_train = predict(RF, newdata = train1[-c(74,75)])
y_pred_test = predict(RF, newdata = test1[-c(74,75)])
# make predictions and summarize accuracy - training set
rf_cm1 = table(train1[,75], y_pred_train)
rf_cm1
sum(diag(rf_cm1))/sum(rf_cm1)
n = sum(rf_cm1) # number of instances
nc = nrow(rf_cm1) # number of classes
diag = diag(rf_cm1) # number of correctly classified instances per class
rowsums = apply(rf_cm1, 1, sum) # number of instances per class
colsums = apply(rf_cm1, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
# make predictions and summarize accuracy - testing set
rf_cm = table(test1[,75], y_pred_test)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
plot(RF)
n = sum(rf_cm) # number of instances
nc = nrow(rf_cm) # number of classes
diag = diag(rf_cm) # number of correctly classified instances per class
rowsums = apply(rf_cm, 1, sum) # number of instances per class
colsums = apply(rf_cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred_train = predict(RF, newdata = train1[-c(74,75)])
y_pred_test = predict(RF, newdata = test1[-c(74,75)])
# make predictions and summarize accuracy - training set
rf_cm1 = table(train1[,75], y_pred_train)
rf_cm1
sum(diag(rf_cm1))/sum(rf_cm1)
n = sum(rf_cm1) # number of instances
nc = nrow(rf_cm1) # number of classes
diag = diag(rf_cm1) # number of correctly classified instances per class
rowsums = apply(rf_cm1, 1, sum) # number of instances per class
colsums = apply(rf_cm1, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
# make predictions and summarize accuracy - testing set
rf_cm = table(test1[,75], y_pred_test)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
n = sum(rf_cm) # number of instances
nc = nrow(rf_cm) # number of classes
diag = diag(rf_cm) # number of correctly classified instances per class
rowsums = apply(rf_cm, 1, sum) # number of instances per class
colsums = apply(rf_cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
plot(RF)
library(e1071)
classifier = svm(formula = Mean.price ~ . ,data = train1[,-74] ,
type = 'C-classification',
kernel = 'linear')
# make predictions and summarize accuracy - training set
y_pred_train = predict(classifier, newdata = train1[,-c(74,75)])
cm1 = table(train1[,75], y_pred_train)
cm1
sum(diag(cm1))/sum(cm1)
n = sum(cm1) # number of instances
nc = nrow(cm1) # number of classes
diag = diag(cm1) # number of correctly classified instances per class
rowsums = apply(cm1, 1, sum) # number of instances per class
colsums = apply(cm1, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
# Prmake predictions and summarize accuracy - test set
y_pred_test = predict(classifier, newdata = test1[,-c(74,75)])
cm = table(test1[,75], y_pred_test)
cm
sum(diag(cm))/sum(cm)
n = sum(cm) # number of instances
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class
rowsums = apply(cm, 1, sum) # number of instances per class
colsums = apply(cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
library(MASS)
LDA <- lda(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea,data = train1)
#  make predictions and summarize accuracy - training set
y_hat_LDA_train <- predict(LDA, newdata = train1)
summary(y_hat_LDA_train$posterior)
z_LDA_train <- y_hat_LDA_train$class
tbl_LDA1 <- table(train1$Mean.price, z_LDA_train)
tbl_LDA1
sum(diag(tbl_LDA1))/sum(tbl_LDA1)
n = sum(tbl_LDA1) # number of instances
nc = nrow(tbl_LDA1) # number of classes
diag = diag(tbl_LDA1) # number of correctly classified instances per class
rowsums = apply(tbl_LDA1, 1, sum) # number of instances per class
colsums = apply(tbl_LDA1, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
# make predictions and summarize accuracy - testing set
y_hat_LDA_test <- predict(LDA, newdata = test1)
summary(y_hat_LDA_test$posterior)
z_LDA_test <- y_hat_LDA_test$class
tbl_LDA <- table(test1$Mean.price, z_LDA_test)
tbl_LDA
sum(diag(tbl_LDA))/sum(tbl_LDA)
n = sum(tbl_LDA) # number of instances
nc = nrow(tbl_LDA) # number of classes
diag = diag(tbl_LDA) # number of correctly classified instances per class
rowsums = apply(tbl_LDA, 1, sum) # number of instances per class
colsums = apply(tbl_LDA, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
library(VGAM)
# with interaction term
fit_log <- vglm(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea, family=multinomial, data=train1)
# summarize the fit
# make predictions - training set
probabilities <- predict(fit_log, newdata = train1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(train1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(train1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(train1$Mean.price)[3]
# summarize accuracy - training set
tbl_log_train <- table(predictions, train1$Mean.price)
tbl_log_train
sum(diag(tbl_log_train))/sum(tbl_log_train)
n = sum(tbl_log_train) # number of instances
nc = nrow(tbl_log_train) # number of classes
diag = diag(tbl_log_train) # number of correctly classified instances per class
rowsums = apply(tbl_log_train, 1, sum) # number of instances per class
colsums = apply(tbl_log_train, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
# make predictions - testing set
probabilities <- predict(fit_log, newdata = test1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(test1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(test1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(test1$Mean.price)[3]
# summarize accuracy - testing set
tbl_log_test <- table(predictions, test1$Mean.price)
tbl_log_test
sum(diag(tbl_log_test))/sum(tbl_log_test)
n = sum(tbl_log_test) # number of instances
nc = nrow(tbl_log_test) # number of classes
diag = diag(tbl_log_test) # number of correctly classified instances per class
rowsums = apply(tbl_log_test, 1, sum) # number of instances per class
colsums = apply(tbl_log_test, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
ggplot(data=all[!is.na(all$SalePrice),], aes(x=YearBuilt, y=SalePrice))+
geom_point(col='black') +
geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
scale_x_continuous(limits = c(1860 ,2020))+
ggtitle("Sales Price and Year of house built")+
labs(x = 'Year of house built')
