scale_x_continuous(limits = c(100 ,2500)) +
ggtitle("Sales Price and Basement Area")+
labs(x = 'Basement Area (in square feet)')
df <- data.frame(tapply(all$SalePrice, all$Neighborhood, mean))
library(data.table)
setDT(df, keep.rownames = TRUE)[]
colnames(df) <- c('Neighborhood','Average SalePrice')
ggplot(df, aes(x = Neighborhood, y= 'Average SalePrice')) +
geom_col(aes(fill = Neighborhood, name = 'Neighborhood'))+
geom_hline(yintercept=mean(all$SalePrice), linetype="dashed", color = "red")+
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Average Sales Price by Neighborhood")
ggplot(data=all, aes(x=(Neighborhood))) +
geom_histogram(stat='count')+
geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Frequency Distribution of average sales price by neighborhood")
ggplot(data=all[!is.na(all$SalePrice),], aes(x=YearBuilt, y=SalePrice))+
geom_point(col='black') +
geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
scale_x_continuous(limits = c(1860 ,2020))+
ggtitle("Sales Price and Year of house built")+
labs(x = 'Year of house built')
#OLS Model 1 - very naive considering only the variab
# naive.model <- lm(SalePrice ~ Car.miles + OverallQual + OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model)
print("Model 1.0: Car.Time.min")
naive.model0 <- lm(SalePrice ~ Car.Time.min + OverallQual +OverallCond + GrLivArea, data = d_train_noNAs)
summary(naive.model0)
# naive.model1 <- lm(SalePrice ~ Walk.miles + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model1)
naive.model1.1 <- lm(SalePrice ~ Walk.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model1.1)
naive.model2 <- lm(SalePrice ~ Bus.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model2)
m2.train <- lm(SalePrice ~ Bus.time.min + ., data = d_train_noNAs)
#Show only relevant coefficients
a2Pval <- summary(m2.train)$coefficients[1:2, 1:4]
a2Pval
set.seed(12345) # = Seed for replication
#d_train_noNAs <- na.omit(d_train)
x <- model.matrix(SalePrice ~ ., data = d_train_noNAs)[ ,-79]
y <- d_train_noNAs$SalePrice
### We then fit a Lasso regression model (alpha = 1)
fit.lasso <- glmnet(x, y, alpha = 1, family = "gaussian")
plot(fit.lasso, xvar = "lambda", label = TRUE)
### Now we cross-validate
cv.lasso <- cv.glmnet(x, y)
#cv.lasso$lambda.min
#cv.lasso$lambda.1se
#coef(cv.lasso, s = "lambda.min")
plot(cv.lasso)
title(sub = "Graph XX: Cross Validation for Lasso", cex = 1.5)
set.seed(12345) # = Seed for replication
### Extract coefficients corresponding to lambda.min (minimum mean cross-validated error)
myCoefs <- coef(cv.lasso, s = "lambda.1se")
# print(as.matrix(min.coef))
myLasso.Results <- data.frame(
features = myCoefs@Dimnames[[1]][ which(myCoefs != 0 ) ], #intercept included
coefs    = myCoefs              [ which(myCoefs != 0 ) ]  #intercept included
)
# myLasso.Results$level.coefs <- cut(myLasso.Results$coefs, 5, labels = c("lowest","low", "med", "high", "highest"))
#
# table(myLasso.Results$level.coefs)
#
# summary(myLasso.Results$coefs)
#
# ggplot(myLasso.Results) + geom_bar(aes(myLasso.Results$level.coefs, fill = myLasso.Results$level.coefs)) + coord_flip()
pander(myLasso.Results)
#pander(filter(myLasso.Results, level.coefs != "low" ))
myLasso.Results <- myLasso.Results[-1,]
var.lasso <- myLasso.Results[,1]
var.lasso
m4.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = d_train_noNAs)
summary(m4.train)
pref.data <- d_train_noNAs %>%
filter(SalePrice >= 250000)
#& d_train_noNAs$SalePrice > 150000
m5.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = pref.data)
summary(m5.train)
# Neighborhood
train <- read.csv("New_Data.csv")
train <- na.omit(train)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
dropVars <- c('YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotalRmsAbvGrd')
train <- train[,!(names(train) %in% dropVars)]
train <- train[-c(524, 1299, 2550),]
write_csv(train, 'Final_data.csv')
train <- read.csv('Final_data.csv')
# cat('There are', length(numericVars), 'numeric variables, and', length(factorVars), 'categoric variables')
set.seed(123)
quick_RF <- randomForest(x = train[, -76], y = train$SalePrice, ntree = 200,importance = TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
ggplot(imp_DF[1:20,], aes(x = reorder(Variables, MSE), y = MSE, fill = MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position = "none")
train$Mean.price <- cut(train$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )
a <- train%>%
filter(Mean.price == 'Higher')
train1 <- train[1:1092,]
test1 <- train[1092:1447,]
ols <- lm(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea + OverallQual*YearBuilt ,data = train1)
summary(ols)$adj.r.squared
y_hat_ols <- predict(ols, newdata = test1)
y_hat_ols
summary(y_hat_ols)
z_ols <- as.integer(y_hat_ols > 1)
z_ols
table(test1$Mean.price, z_ols)
# step function
train1 <- subset(train1, select = -c(Utilities,Street))
test1 <- subset(test1, select = -c(Utilities,Street))
test1 <- test1[!test1$Id == 1004,]
test1 <- test1[!test1$Id == 1299,]
test1 <- test1[!test1$Id == 1001,]
test1 <- test1[!test1$Id == 1188,]
test1 <- test1[!test1$Id == 411,]
test1 <- test1[!test1$Id == 251,]
test1 <- test1[!test1$Id == 596,]
library(VGAM)
# with interaction term
fit_log <- vglm(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea, family=multinomial, data=train1)
# summarize the fit
# make predictions - training set
probabilities <- predict(fit_log, newdata = train1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(train1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(train1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(train1$Mean.price)[3]
# summarize accuracy - training set
tbl_log <- table(predictions, train1$Mean.price)
tbl_log
sum(diag(tbl_log))/sum(tbl_log)
# make predictions - testing set
probabilities <- predict(fit_log, newdata = test1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(test1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(test1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(test1$Mean.price)[3]
# summarize accuracy - testing set
tbl_log <- table(predictions, test1$Mean.price)
tbl_log
sum(diag(tbl_log))/sum(tbl_log)
library(MASS)
LDA <- lda(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea,data = train1)
#  make predictions and summarize accuracy - training set
y_hat_LDA <- predict(LDA, newdata = train1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA <- table(train1$Mean.price, z_LDA)
tbl_LDA
sum(diag(tbl_LDA))/sum(tbl_LDA)
# make predictions and summarize accuracy - testing set
y_hat_LDA <- predict(LDA, newdata = test1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA <- table(test1$Mean.price, z_LDA)
tbl_LDA
sum(diag(tbl_LDA))/sum(tbl_LDA)
library(e1071)
classifier = svm(formula = Mean.price ~ . ,data = train1[,-74] ,
type = 'C-classification',
kernel = 'linear')
help("svm")
# make predictions and summarize accuracy - training set
y_pred = predict(classifier, newdata = train1[,-c(74,75)])
cm = table(train1[,75], y_pred)
cm
sum(diag(cm))/sum(cm)
# Prmake predictions and summarize accuracy - test set
y_pred = predict(classifier, newdata = test1[,-c(74,75)])
cm = table(test1[,75], y_pred)
cm
sum(diag(cm))/sum(cm)
RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred1 = predict(RF, newdata = train1[-c(74,75)])
y_pred2 = predict(RF, newdata = test1[-c(74,75)])
# make predictions and summarize accuracy - training set
rf_cm = table(train1[,75], y_pred1)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
# make predictions and summarize accuracy - testing set
rf_cm = table(test1[,75], y_pred2)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
plot(RF)
df <- data.frame(tapply(all$SalePrice, all$Neighborhood, mean))
library(data.table)
setDT(df, keep.rownames = TRUE)[]
colnames(df) <- c('Neighborhood','Average SalePrice')
ggplot(df, aes(x = Neighborhood, y= 'Average SalePrice')) +
geom_col(aes(fill = Neighborhood, name = 'Neighborhood'))+
geom_hline(yintercept=mean(all$SalePrice), linetype="dashed", color = "red")+
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Average Sales Price by Neighborhood")
ggplot(data=all, aes(x=(Neighborhood))) +
geom_histogram(stat='count')+
geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Frequency Distribution of average sales price by neighborhood")
df <- data.frame(tapply(all$SalePrice, all$Neighborhood, mean))
library(data.table)
setDT(df, keep.rownames = TRUE)[]
colnames(df) <- c('Neighborhood','Average SalePrice')
ggplot(data=all, aes(x=(Neighborhood))) +
geom_histogram(stat='count')+
geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Frequency Distribution of average sales price by neighborhood")
#plot the results
ggplot(neighb) + geom_col(aes(neighb$rn, y = neighb$mean.price,  fill = df$rn)) +
coord_flip() +
ggtitle("Average Sales Price by Neighborhood") +
ylab("Average Sales Price") + scale_y_continuous(labels = scales::dollar) +
xlab("Neighborhood") + coord_flip()
#d_train1 <- d_train[,-c(73, 75, 7, 74, 57,4, 13, 83,84)]
d_train_noNAs <- na.omit(d_train)
df <- data.frame(tapply(d_train_noNAs$SalePrice, d_train_noNAs$Neighborhood, mean))
library(data.table)
library(forcats)
neighb <- setDT(df, keep.rownames = TRUE)[]
colnames(neighb)[2] <- "mean.price"
#Order values in ascending order.
neighb <- neighb[order(mean.price), ]
neighb$rn <- factor(neighb$rn, levels=unique(as.character(neighb$rn)) )
#Create categories for mean sales price of the neighborhood.
d_train_noNAs$Mean.price <- cut(d_train_noNAs$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )
#plot the results
ggplot(neighb) + geom_col(aes(neighb$rn, y = neighb$mean.price,  fill = df$rn)) +
coord_flip() +
ggtitle("Average Sales Price by Neighborhood") +
ylab("Average Sales Price") + scale_y_continuous(labels = scales::dollar) +
xlab("Neighborhood") + coord_flip()
ggplot(data=all, aes(x=(Neighborhood))) +
geom_histogram(stat='count')+
geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Distribution of houses count by neighborhood")
ggplot(data=all[!is.na(all$SalePrice),], aes(x=YearBuilt, y=SalePrice))+
geom_point(col='black') +
geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
scale_x_continuous(limits = c(1860 ,2020))+
ggtitle("Sales Price and Year of house built")+
labs(x = 'Year of house built')
#OLS Model 1 - very naive considering only the variab
# naive.model <- lm(SalePrice ~ Car.miles + OverallQual + OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model)
print("Model 1.0: Car.Time.min")
naive.model0 <- lm(SalePrice ~ Car.Time.min + OverallQual +OverallCond + GrLivArea, data = d_train_noNAs)
summary(naive.model0)
# naive.model1 <- lm(SalePrice ~ Walk.miles + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model1)
naive.model1.1 <- lm(SalePrice ~ Walk.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model1.1)
naive.model2 <- lm(SalePrice ~ Bus.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model2)
m2.train <- lm(SalePrice ~ Bus.time.min + ., data = d_train_noNAs)
#Show only relevant coefficients
a2Pval <- summary(m2.train)$coefficients[1:2, 1:4]
a2Pval
set.seed(12345) # = Seed for replication
#d_train_noNAs <- na.omit(d_train)
x <- model.matrix(SalePrice ~ ., data = d_train_noNAs)[ ,-79]
y <- d_train_noNAs$SalePrice
### We then fit a Lasso regression model (alpha = 1)
fit.lasso <- glmnet(x, y, alpha = 1, family = "gaussian")
plot(fit.lasso, xvar = "lambda", label = TRUE)
### Now we cross-validate
cv.lasso <- cv.glmnet(x, y)
#cv.lasso$lambda.min
#cv.lasso$lambda.1se
#coef(cv.lasso, s = "lambda.min")
plot(cv.lasso)
title(sub = "Graph XX: Cross Validation for Lasso", cex = 1.5)
set.seed(12345) # = Seed for replication
### Extract coefficients corresponding to lambda.min (minimum mean cross-validated error)
myCoefs <- coef(cv.lasso, s = "lambda.1se")
# print(as.matrix(min.coef))
myLasso.Results <- data.frame(
features = myCoefs@Dimnames[[1]][ which(myCoefs != 0 ) ], #intercept included
coefs    = myCoefs              [ which(myCoefs != 0 ) ]  #intercept included
)
# myLasso.Results$level.coefs <- cut(myLasso.Results$coefs, 5, labels = c("lowest","low", "med", "high", "highest"))
#
# table(myLasso.Results$level.coefs)
#
# summary(myLasso.Results$coefs)
#
# ggplot(myLasso.Results) + geom_bar(aes(myLasso.Results$level.coefs, fill = myLasso.Results$level.coefs)) + coord_flip()
pander(myLasso.Results)
#pander(filter(myLasso.Results, level.coefs != "low" ))
myLasso.Results <- myLasso.Results[-1,]
var.lasso <- myLasso.Results[,1]
var.lasso
m4.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = d_train_noNAs)
summary(m4.train)
pref.data <- d_train_noNAs %>%
filter(SalePrice >= 250000)
#& d_train_noNAs$SalePrice > 150000
m5.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = pref.data)
summary(m5.train)
# Neighborhood
train <- read.csv("New_Data.csv")
train <- na.omit(train)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use = "pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
#select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
train <- read.csv('Final_data.csv')
# cat('There are', length(numericVars), 'numeric variables, and', length(factorVars), 'categoric variables')
set.seed(123)
quick_RF <- randomForest(x = train[, -76], y = train$SalePrice, ntree = 200,importance = TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
ggplot(imp_DF[1:20,], aes(x = reorder(Variables, MSE), y = MSE, fill = MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position = "none")
train <- read.csv('Final_data.csv')
# cat('There are', length(numericVars), 'numeric variables, and', length(factorVars), 'categoric variables')
set.seed(123)
quick_RF <- randomForest(x = train[, -76], y = train$SalePrice, ntree = 200,importance = TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
ggplot(imp_DF[1:20,], aes(x = reorder(Variables, MSE), y = MSE, fill = MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position = "none")
title(sub = "Graph XX: Cross Validation for Lasso", cex = 1.5)
ggplot(imp_DF[1:20,], aes(x = reorder(Variables, MSE), y = MSE, fill = MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position = "none")+ title(sub = "Graph XX: Cross Validation for Lasso", cex = 1.5)
ggplot(imp_DF[1:20,], aes(x = reorder(Variables, MSE), y = MSE, fill = MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position = "none") + title(sub = "Graph XX: Cross Validation for Lasso", cex = 1.5)
#plot the results
ggplot(neighb) + geom_col(aes(neighb$rn, y = neighb$mean.price,  fill = df$rn)) +
coord_flip() +
ggtitle("Average Sales Price by Neighborhood") +
ylab("Average Sales Price") + scale_y_continuous(labels = scales::dollar) +
xlab("Neighborhood") + coord_flip()
#Distribution of houses per neighborhood category
pander(table(d_train_noNAs$Neighborhood, d_train_noNAs$Mean.price))
summary(all$SalePrice)
hist(all$SalePrice,
main = 'Histogram of Sale Price',
xlab = 'Sale Price',
breaks=seq(30000, 760000, 5000),prob=TRUE)
lines(density(all$SalePrice), col="blue", lwd=2)
lines(density(all$SalePrice, adjust=2), lty="dotted")
ggplot(data=all[!is.na(all$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1)) +
scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma) +
ggtitle("Sales Price and Ground Living Area")+
labs(x = 'Ground Living Area (in square feet)')
# potential outliers found
outlier1 <- all%>% filter(GrLivArea > 4500)
outlier1$Id
all[c(524, 1299, 2550), c('SalePrice', 'GrLivArea', 'OverallQual')]
#OLS Model 1 - very naive considering only the variab
# naive.model <- lm(SalePrice ~ Car.miles + OverallQual + OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model)
print("Model 1.0: Car.Time.min")
naive.model0 <- lm(SalePrice ~ Car.Time.min + OverallQual +OverallCond + GrLivArea, data = d_train_noNAs)
summary(naive.model0)
set.seed(12345) # = Seed for replication
#d_train_noNAs <- na.omit(d_train)
x <- model.matrix(SalePrice ~ ., data = d_train_noNAs)[ ,-79]
y <- d_train_noNAs$SalePrice
### We then fit a Lasso regression model (alpha = 1)
fit.lasso <- glmnet(x, y, alpha = 1, family = "gaussian")
plot(fit.lasso, xvar = "lambda", label = TRUE)
### Now we cross-validate
cv.lasso <- cv.glmnet(x, y)
#cv.lasso$lambda.min
#cv.lasso$lambda.1se
#coef(cv.lasso, s = "lambda.min")
plot(cv.lasso)
title(sub = "Graph XX: Cross Validation for Lasso", cex = 1.5)
set.seed(12345) # = Seed for replication
### Extract coefficients corresponding to lambda.min (minimum mean cross-validated error)
myCoefs <- coef(cv.lasso, s = "lambda.1se")
# print(as.matrix(min.coef))
myLasso.Results <- data.frame(
features = myCoefs@Dimnames[[1]][ which(myCoefs != 0 ) ], #intercept included
coefs    = myCoefs              [ which(myCoefs != 0 ) ]  #intercept included
)
# myLasso.Results$level.coefs <- cut(myLasso.Results$coefs, 5, labels = c("lowest","low", "med", "high", "highest"))
#
# table(myLasso.Results$level.coefs)
#
# summary(myLasso.Results$coefs)
#
# ggplot(myLasso.Results) + geom_bar(aes(myLasso.Results$level.coefs, fill = myLasso.Results$level.coefs)) + coord_flip()
pander(myLasso.Results)
#pander(filter(myLasso.Results, level.coefs != "low" ))
myLasso.Results <- myLasso.Results[-1,]
var.lasso <- myLasso.Results[,1]
var.lasso
m4.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = d_train_noNAs)
summary(m4.train)
pref.data <- d_train_noNAs %>%
filter(SalePrice >= 250000)
#& d_train_noNAs$SalePrice > 150000
m5.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = pref.data)
summary(m5.train)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use = "pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
#select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred1 = predict(RF, newdata = train1[-c(74,75)])
y_pred2 = predict(RF, newdata = test1[-c(74,75)])
# make predictions and summarize accuracy - training set
rf_cm = table(train1[,75], y_pred1)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
# make predictions and summarize accuracy - testing set
rf_cm = table(test1[,75], y_pred2)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
plot(RF)
# make predictions - training set
probabilities <- predict(fit_log, newdata = train1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(train1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(train1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(train1$Mean.price)[3]
# summarize accuracy - training set
tbl_log1 <- table(predictions, train1$Mean.price)
tbl_log1
sum(diag(tbl_log1))/sum(tbl_log1)
# make predictions - testing set
probabilities <- predict(fit_log, newdata = test1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(test1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(test1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(test1$Mean.price)[3]
# summarize accuracy - testing set
tbl_log2 <- table(predictions, test1$Mean.price)
tbl_log2
sum(diag(tbl_log2))/sum(tbl_log2)
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_log1))/sum(tbl_log1) , 'The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_log2))/sum(tbl_log2))
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_log1))/sum(tbl_log1))
cat('The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_log2))/sum(tbl_log2))
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_log1))/sum(tbl_log1))
cat('The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_log2))/sum(tbl_log2))
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_log1))/sum(tbl_log1) '; The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_log2))/sum(tbl_log2))
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_log1))/sum(tbl_log1)'; The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_log2))/sum(tbl_log2))
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_log1))/sum(tbl_log1)', The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_log2))/sum(tbl_log2))
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_log1))/sum(tbl_log1), ', The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_log2))/sum(tbl_log2))
tbl_LDA1 <- table(train1$Mean.price, z_LDA)
library(MASS)
LDA <- lda(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea,data = train1)
#  make predictions and summarize accuracy - training set
y_hat_LDA <- predict(LDA, newdata = train1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA1 <- table(train1$Mean.price, z_LDA)
tbl_LDA1
sum(diag(tbl_LDA1))/sum(tbl_LDA1)
# make predictions and summarize accuracy - testing set
y_hat_LDA <- predict(LDA, newdata = test1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA <- table(test1$Mean.price, z_LDA)
tbl_LDA
sum(diag(tbl_LDA))/sum(tbl_LDA)
cat('The prediction accuracy of Vector Generalized Linear Models in training set is', sum(diag(tbl_LDA1))/sum(tbl_LDA1), ', The prediction accuracy of Vector Generalized Linear Models in testing set is, and', sum(diag(tbl_LDA))/sum(tbl_LDA))
library(e1071)
classifier = svm(formula = Mean.price ~ . ,data = train1[,-74] ,
type = 'C-classification',
kernel = 'linear')
help("svm")
# make predictions and summarize accuracy - training set
y_pred = predict(classifier, newdata = train1[,-c(74,75)])
cm1 = table(train1[,75], y_pred)
cm1
sum(diag(cm1))/sum(cm1)
# Prmake predictions and summarize accuracy - test set
y_pred = predict(classifier, newdata = test1[,-c(74,75)])
cm = table(test1[,75], y_pred)
cm
sum(diag(cm))/sum(cm)
cat('The prediction accuracy of Support Vector Machine Models in training set is', sum(diag(cm1))/sum(cm1), ', The prediction accuracy of Support Vector Machine Models in testing set is,', sum(diag(cm))/sum(cm))
RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred1 = predict(RF, newdata = train1[-c(74,75)])
y_pred2 = predict(RF, newdata = test1[-c(74,75)])
# make predictions and summarize accuracy - training set
rf_cm1 = table(train1[,75], y_pred1)
rf_cm1
sum(diag(rf_cm1))/sum(rf_cm1)
# make predictions and summarize accuracy - testing set
rf_cm = table(test1[,75], y_pred2)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
plot(RF)
cat('The prediction accuracy of Support Vector Machine Models in training set is', sum(diag(rf_cm1))/sum(rf_cm1), ', The prediction accuracy of Support Vector Machine Models in testing set is', sum(diag(rf_cm))/sum(rf_cm))
cm
plot(y_pred)
y_pred$posterior
summary(y_pred)
