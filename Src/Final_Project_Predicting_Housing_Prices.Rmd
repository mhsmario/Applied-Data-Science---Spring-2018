---
title: "QMSS5069 - Advanced Data Science Final Project: Predicting Housing Prices"
author: "Lizhizi Cui (lc3268) and Mario Saraiva (mhs2195)"
date: "May 2018"
output: 
    html_document:
    code_folding: hide
    toc: yes
    toc_float: yes
---

<center>
`Applied Data Science for Social Sciences Project: Preliminary results`
  
`Lizhizi Cui (lc3268) and Mario Saraiva (mhs2195)`
  
`March 28, 2018`
</center>

<p>
  
  
</p> 

```{r, include=FALSE }
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

##Introduction

#### Overview 

This is a consulting project where our main goal is to advise our client on where
he should buy a house and how much a house he likes may cost. Our client, a 
professor at the Utah State University, approached our team seeking for advice on
where he should buy his next house in hopes to sell it in a couple of years and
profit out of the transaction. The professor has a strict preference for houses
within the city of Ames, IA and he is not interested buying anywhere else.

To best attend to our clients request, we explored the housing data on Ames, IA 
through different visualizations and a correlation plot to understand the dataset
[^1]. Then different inference models were constructured to verify our hypothesis. 
Once the hypothesis had been tested we proceeded to the predictive modeling section,
where through various machine learning models we predict the final sales price of
a house. The last section of this document presents our conclusions and
recommendations.

[^1]: https://www.kaggle.com/c/house-prices-advanced-regression-techniques  

#### 1. Housing Data

The dataset contains information on 1460 houses in Ames, Iowa. There are 81 
variables in total, of which 37 are quantitative including the final Sales Price
of the house, 43 are categorical + the Id number of each house. For more additional
details on the available variables please refer to the annex section.

```{r setup, include=FALSE}
library(glmnet)
library(purrr)
library(tidyr)
library(readr)
library(corrplot)
library(shiny)
library(dplyr)
library(ggplot2)
library(e1071)
library(glmnet)
library(mice)
library(lattice)
library(VIM)
library(caret)
library(stargazer)
library(gam)
library(flam)
library(bartMachine)
library(tree)
library(ISLR)
library(RColorBrewer)
library(ggfortify)
library(scales)
library(stargazer)
library(rsconnect)
library(knitr)
library(gridExtra)
library(scales)
library(Rmisc)
library(ggrepel)
library(randomForest)
library(psych)
library(purrr)
library(tidyr)
library(ggplot2)
library(readr)
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(shiny)
library(ggfortify)
library(pander)
library(purrr)
library(tidyr)
library(ggplot2)
library(readr)
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(shiny)
library(ggfortify)
library(pander)
library(here)
library(devtools)
```

```{r}
#all <- read.csv("train.csv")
all <- read.csv("/Users/lizhizicui/Desktop/Final-Project---Applied-Data-Science/Data/Raw/train.csv")
set.seed(12345)
in_train <- createDataPartition(y = all$SalePrice, p = 3 / 4, list = FALSE) # Split data into training and testing and recombine them
str(in_train)
training <- all[ in_train, ] # three fourth of data go to training
testing  <- all[-in_train, ]

all <- rbind(training, testing)
```

### 1.1  Missing values
 
We noticed a significant number of missing variables in the dataset. The variables with the highest number of missingness were: "PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", "LotFrontage". Table X summarizes our findings.
 
```{r, warning=FALSE, message=FALSE}
na_count <-sapply(all, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
```

<center>

`Table X: Missing Data`

`r pander(na_count)`

</center>

### 1.2 Dealing with missing values

We assessed the missing values and identified the ones that were not missing at random.
These were modified separately and then a new dataset containing 1451 observations and 79 variables was created.

```{r, dealing with NAs}
all_test <- all

# Add new levels and change NA into 'None' level
addLevel <- function(x, newlevel=NULL) {
  if(is.factor(x)) {
    if (is.na(match(newlevel, levels(x))))
      return(factor(x, levels=c(levels(x), newlevel)))
  }
  return(x)
}
all_test$PoolQC <- addLevel(all_test$PoolQC, "None")
all_test$PoolQC <- addLevel(all_test$PoolQC, "Po")
all_test$PoolQC <- addLevel(all_test$PoolQC, "TA")
all_test$PoolQC <- ifelse(is.na(all$PoolQC), "None", paste(all$PoolQC))

Qualities <- c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
all_test$PoolQC<- revalue(all_test$PoolQC, Qualities)
all_test$PoolQC <- as.factor(all_test$PoolQC)
```

```{r dealing with NAs 2}

# Add new levels and change NA into 'None' level, meaning No Miscellaneous Feature
all_test$MiscFeature <- addLevel(all_test$MiscFeature, "None")
all_test$MiscFeature <- ifelse(is.na(all_test$MiscFeature), "None", paste(all_test$MiscFeature))
all_test$MiscFeature <- as.factor(all_test$MiscFeature)

# Add new levels and change NA into 'None' level, meaning No Alley                                                                         
all_test$Alley <- addLevel(all_test$Alley, "None")
all_test$Alley <- ifelse(is.na(all_test$Alley), "None", paste(all$Alley))
all_test$Alley <- as.factor(all_test$Alley)

# Add new levels and change NA into 'None' level, meaning No Fence
all_test$Fence <- addLevel(all_test$Fence, "None")
all_test$Fence <- ifelse(is.na(all$Fence), "None", paste(all_test$Fence))
all_test$Fence <- as.factor(all_test$Fence)

# Add new levels and change NA into 'None' level, meaning No Fire place
all_test$FireplaceQu <- addLevel(all_test$FireplaceQu, "None")
all_test$FireplaceQu <- ifelse(!is.na(all_test$FireplaceQu), paste(all_test$FireplaceQu), "None")
all_test$FireplaceQu <- as.factor(all_test$FireplaceQu)

# Add new levels and change NA into 'None' level, meaning No Garage
all_test$GarageType <- addLevel(all_test$GarageType, "None")
all_test$GarageType <- ifelse(is.na(all_test$GarageType), "None", paste(all_test$GarageType))
all_test$GarageType <- as.factor(all_test$GarageType)

# Add new levels and change NA into 'No Garage', meaning No Garage
all_test$GarageYrBlt[is.na(all_test$GarageYrBlt)] <- 'No Garage'
all$GarageYrBlt <- as.numeric(all$GarageYrBlt)

# Add new levels and change NA into 'No Garage', meaning No Garage
all_test$GarageFinish <- addLevel(all_test$GarageFinish, "No Garage")
all_test$GarageFinish <- ifelse(is.na(all_test$GarageFinish), "No Garage", paste(all_test$GarageFinish))
all_test$GarageFinish <- as.factor(all_test$GarageFinish)

# Add new levels and change NA into 'No Garage', meaning No Garage
all_test$GarageCars[is.na(all_test$GarageCars)] <- 'No Garage'
all_test$GarageCars <- as.factor(all_test$GarageCars)

# Add new levels and change NA into 'No Garage', meaning No Garage
all_test$GarageArea[is.na(all_test$GarageArea)] <- 'No Garage'

# Add new levels and change NA into 'None' level, meaning No Garage
all_test$GarageQual <- addLevel(all_test$GarageQual, "None")
all_test$GarageQual <- ifelse(is.na(all_test$GarageQual), "None", paste(all_test$GarageQual))
all_test$GarageQual <- as.factor(all_test$GarageQual)

# Add new levels and change NA into 'None' level, meaning No Garage
all_test$GarageCond <- addLevel(all_test$GarageCond, "None")
all_test$GarageCond <- ifelse(is.na(all_test$GarageCond), "None", paste(all_test$GarageCond))
all_test$GarageCond <- as.factor(all_test$GarageCond)

# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtQual <- addLevel(all_test$BsmtQual, "None")
all_test$BsmtQual <- ifelse(is.na(all_test$BsmtQual), "None", paste(all_test$BsmtQual))
all_test$BsmtQual <- as.factor(all_test$BsmtQual)

# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtExposure <- addLevel(all_test$BsmtExposure, "None")
all_test$BsmtExposure <- ifelse(is.na(all_test$BsmtExposure), "None", paste(all_test$BsmtExposure))
all_test$BsmtExposure <- as.factor(all_test$BsmtExposure)

# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtFinType1 <- addLevel(all_test$BsmtFinType1, "None")
all_test$BsmtFinType1 <- ifelse(is.na(all_test$BsmtFinType1), "None", paste(all_test$BsmtFinType1))
all_test$BsmtFinType1 <- as.factor(all_test$BsmtFinType1)

# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtCond <- addLevel(all_test$BsmtCond, "None")
all_test$BsmtCond <- ifelse(is.na(all_test$BsmtCond), "None", paste(all_test$BsmtCond))
all_test$BsmtCond <- as.factor(all_test$BsmtCond)

# Add new levels and change NA into 'None' level, meaning No Basement
all_test$BsmtFinType2 <- addLevel(all_test$BsmtFinType2, "None")
all_test$BsmtFinType2 <- ifelse(is.na(all_test$BsmtFinType2), "None", paste(all_test$BsmtFinType2))
all_test$BsmtFinType2 <- as.factor(all_test$BsmtFinType2)

# Set Basement Area equals 0 for houses which do not have basement
all_test$BsmtFinSF1[is.na(all_test$BsmtFinSF1)] <-0
all_test$BsmtFinSF2[is.na(all_test$BsmtFinSF2)] <-0
all_test$BsmtUnfSF[is.na(all_test$BsmtUnfSF)] <-0

# Set Basement Area equals 0 for houses which has zero linear feet of street connected to property
all_test$LotFrontage[is.na(all_test$LotFrontage)] <-0

drop.na.columns <- c( "MasVnrType","LotFrontage")
all_test<- all_test[ , !(names(all_test) %in% drop.na.columns)]
all_test <- na.omit(all_test)

```

```{r dealing with NAs 3}

all <- all_test

# factorize some of the variables
all$Foundation <- as.factor(all$Foundation)
all$Heating <- as.factor(all$Heating)
all$RoofStyle <- as.factor(all$RoofStyle)
all$RoofMatl <- as.factor(all$RoofMatl)
all$LandContour <- as.factor(all$LandContour)
all$BldgType <- as.factor(all$BldgType)
all$HouseStyle <- as.factor(all$HouseStyle)
all$Condition1 <- as.factor(all$Condition1)
all$Condition2 <- as.factor(all$Condition2)
all$GarageArea <- as.numeric(all$GarageArea)

na_count_test <-sapply(all, function(y) sum(length(which(is.na(y)))))
na_count_test <- data.frame(na_count_test)


write_excel_csv(all, "New_Data.csv")
```

## 2. Inference 

In this section we investigated the relationship between House sales Price and 
distance from Utah State University. Distance between the house is an important
factor to our client. The professor emphasized that at this stage he wants to 
find out if it is better to buy a house near campus or not. 

### 2.1 Hypothesis

Our initial hypothesis is that there is a positive association 
between final sales price and distance from ISU. In other words, houses that are
further away from the university are, in general, sold for a higher price than 
those in neighborhoods closer to ISU. Without doing any data analysis, we know 
that there is a golf course at the opposite end of city from the University, 
which could be evidence supporting our hypothesis. Additionally, student housing
usually has a lower price in comparison with other non-student houses, if this 
is true in Ames, then our hypothesis should be correct.

<center>
  __Hypothesis:__ 
</center>
  
$$Sales Price = \beta_0 + \beta_{Dist. ISU} + \beta_{House Quality} + \beta_{Size} + ... + \mu$$
Where $\beta_{Dist. ISU}$ is positive. 

```{r, message=FALSE, warning=FALSE}
#Load treated and processed data 'New_Data.csv'
New_Data <- read_csv("New_Data.csv")
#Load the data with distance between ISU and

#distance <- read_csv("distance.csv")
distance <- read_csv("/Users/lizhizicui/Desktop/Final-Project---Applied-Data-Science/Data/Processed/distance.csv")
#Add the distance data to both Train and Test datasets.
d_train <- left_join(New_Data, distance, by = "Neighborhood")
```

```{r message=FALSE, warning=FALSE}
#d_train1 <- d_train[,-c(73, 75, 7, 74, 57,4, 13, 83,84)]

d_train_noNAs <- na.omit(d_train)

df <- data.frame(tapply(d_train_noNAs$SalePrice, d_train_noNAs$Neighborhood, mean))

library(data.table)
library(forcats)

neighb <- setDT(df, keep.rownames = TRUE)[]
colnames(neighb)[2] <- "mean.price"

#Order values in ascending order.
neighb <- neighb[order(mean.price), ]
neighb$rn <- factor(neighb$rn, levels=unique(as.character(neighb$rn)) )


  
#Create categories for mean sales price of the neighborhood.
  
d_train_noNAs$Mean.price <- cut(d_train_noNAs$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )

```

###2.2 Context

Ames, IA located in the midwest of the United States is predominantly a college city. Ames has a population of 62,815 people with a median age of 23.2. The city is home for the Iowa State University (ISU) and its 35,000 students (excluding postdoc students and faculty members). Not surprisingly, the city has a median household income of $41,616. 

According to Data USA, the median property value in Ames, IA is $174,300, and the homeownership rate is 41%. The average commute time is 15.7 minutes. The average car ownership in Ames, IA is 2 cars per household. [^2]

[^2]: https://datausa.io/profile/geo/ames-ia/ 


From the data we can see that three neighborhoods - NoRidge, NridgHt, and StoneBr
- have the highest average sales price, \$300,000 or more. We classified these 
three neighborhoods as "High", in regards to their average sales price. Similarly,
we identified two other stratas of neighborhoods one as "Medium" with the average
sales price between $150,000 and \$300,000, and "Low" all neighborhoods with 
average sales price up to $150,000.

  
```{r}
#plot the results
ggplot(neighb) + geom_col(aes(neighb$rn, y = neighb$mean.price,  fill = df$rn)) + 
  coord_flip() + 
  ggtitle("Average Sales Price by Neighborhood") +
  ylab("Average Sales Price") + scale_y_continuous(labels = scales::dollar) +
  xlab("Neighborhood") + coord_flip()
```
Table 1.1 summarizes the the average sales price for each neighborhood in Ames, Iowa.

```{r}
ggplot(data=all, aes(x=(Neighborhood))) +
        geom_histogram(stat='count')+
        geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
        ggtitle("Distribution of houses count by neighborhood")
```


Table 1.2 summarizes the number of houses and their average price per neighborhood.

<center>

`Table XX`

```{r}
#Distribution of houses per neighborhood category
pander(table(d_train_noNAs$Neighborhood, d_train_noNAs$Mean.price))
```

</center>

###2.3 Hypothesis
  
 As seen in the 'Context' section, the city of Ames is a college city, with approximately 35,993 students (exluding postdocs), accouting for more than 57% of the population. This is a conservative estimate since we are not including the university's staff. Arguebly, at least 60% of Ames works and/or studies at the Iowa State University.

`Then, what is the relationship between distance from the university and the final sales price of a house?`

Our dataset does not have the exact location of each house, so we used the average distance between the approximate center point of each neighborhood to the center of IUS campus. We used google maps to calculate the distance in miles by car and walking, as well as, the time it would take to go from the location to the IUS campus. Since the campus is fairly large we used Global Café, 513 Farm House Ln, Ames, IA, 50010, as our reference point representing the University. See annex for a full description of our distance 
variables.


### 2.4 Understanding key variables 

<h4> __SalePrice__ </h4>

SalePrice: the property's sale price in dollars
 
```{r SalesPrice}
summary(all$SalePrice)

hist(all$SalePrice, 
     main = 'Histogram of Sale Price',
     xlab = 'Sale Price',
     breaks=seq(30000, 760000, 5000),prob=TRUE)
lines(density(all$SalePrice), col="blue", lwd=2)       
lines(density(all$SalePrice, adjust=2), lty="dotted")

```
Table 2.1 summarizes the frequency of Sales Price for all houses.


<h4> __Overall Quality__ </h4>
  
OverallQual: Rates the overall material and finish of the house

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor
       
Overall Quality is observed to have positive correlation with SalePrice. It does not seem to have noticeble outliers apart from a point in level 4. 

```{r}
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(OverallQual), y=SalePrice))+
        geom_boxplot() + labs(x='Overall Quality') +
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma) +
        ggtitle("Boxplot - Sales Price and Overall Quality")
```
Table 2.2 summarizes the relationship of Sales Price and Overall Quality Rating of houses.

<h4> __OverallCond__ </h4>

OverallCond: Rates the overall condition of the house:

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average	
       5	Average
       4	Below Average	
       3	Fair
       2	Poor
       1	Very Poor
       

```{r SalesPrice and Condition}
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(OverallCond), y=SalePrice))+
        geom_boxplot() + labs(x='Overall Condition')+
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma) +
  ggtitle("Boxplot - Sales Price and Overall Condition") 
```
Table 2.3 summarizes the relationship of Sales Price and Overall Condition Rating of houses.

<h4> __'GrLivArea' (Above Ground Living Area)__ </h4>

GrLivArea: Above ground living area square feet  
GrLivArea is observed to have positive correlation with SalePrice, which make sense since big houses are generally more expensive than smaller. One thing worth noticing is that there are two houses with really large living areas and low SalePrice (potential to be outliers). I checked the data and found out they are houses 524, 1299 and 2550(NA for SalePrice).

```{r}
ggplot(data=all[!is.na(all$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
  geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1)) +
  scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma) +
  ggtitle("Sales Price and Ground Living Area")+
  labs(x = 'Ground Living Area (in square feet)')

# potential outliers found
outlier1 <- all%>% filter(GrLivArea > 4500)
outlier1$Id
all[c(524, 1299, 2550), c('SalePrice', 'GrLivArea', 'OverallQual')]

```
Table 2.4 summarizes the relationship of Sales Price and Ground Living Area of houses.

<h4> __GarageCars__ </h4>
  
The Size of garage shows a positive correlation with SalePrice from level 0-3 but fall at level 4 (non-linear correlation). It make sense because only big houses have large garage, and bigger house usually have higher SalePrice. Also for houses with a garage of capacity of 3 cars(level 3), there are several outliners with much higher SalePrice existing, it must be other factors affecting the outcome price. Something might happen for houses with level 4 garage that needed to explained.

```{r, GarageCars}
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(GarageCars), y=SalePrice))+
        geom_boxplot() + labs(x='Size of Garage/car') +
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma)+
  ggtitle("Sales Price and Garage Capacity")

#potential outliers found
outlier2 <- all %>% 
  filter(GarageCars == 3)%>%
  filter(SalePrice > 700000)

outlier2$Id

#check the other dimention information of outliers 
all[c(524, 1299, 692, 1183), c('SalePrice', 'GrLivArea', 'OverallQual','GarageCars')] 

```
Table 2.5 summarizes the relationship of Sales Price and Capacity of Garage of houses.

<h4> __BsmtFinSF1__ </h4>

BsmtFinSF1: Type 1 basement finished square feet
```{r, BsmtFinSF1}

ggplot(data=all[!is.na(all$SalePrice),], aes(x=BsmtFinSF1, y=SalePrice))+
        geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
        scale_x_continuous(limits = c(100 ,2500)) + 
  ggtitle("Sales Price and Basement Area")+
  labs(x = 'Basement Area (in square feet)')
  

```
Table 2.6 summarizes the relationship of Sales Price and Basement Area of houses.

<h4> __Neighborhood__ </h4>

Neighborhood: Physical locations within Ames city limits

       Blmngtn	Bloomington Heights
       Blueste	Bluestem
       BrDale	Briardale
       BrkSide	Brookside
       ClearCr	Clear Creek
       CollgCr	College Creek
       Crawfor	Crawford
       Edwards	Edwards
       Gilbert	Gilbert
       IDOTRR	Iowa DOT and Rail Road
       MeadowV	Meadow Village
       Mitchel	Mitchell
       Names	North Ames
       NoRidge	Northridge
       NPkVill	Northpark Villa
       NridgHt	Northridge Heights
       NWAmes	Northwest Ames
       OldTown	Old Town
       SWISU	South & West of Iowa State University
       Sawyer	Sawyer
       SawyerW	Sawyer West
       Somerst	Somerset
       StoneBr	Stone Brook
       Timber	Timberland
       Veenker	Veenker


<h4> __YearBuilt__ </h4>
YearBuilt: Original construction date
```{r, yearbuilt}

ggplot(data=all[!is.na(all$SalePrice),], aes(x=YearBuilt, y=SalePrice))+
  geom_point(col='black') + 
  geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
  scale_x_continuous(limits = c(1860 ,2020))+
  ggtitle("Sales Price and Year of house built")+
  labs(x = 'Year of house built')

```
Table 2.7 summarizes the relationship of Sales Price and Year of house built.

### 2.3 Inferential Models

We will investigate the validity of our hypothesis through using Ordinary Least 
Squares to generate three different models.

####2.3.1 Ordinary Least Squares

First we ran a naive OLS model including only the variables of interest in the 
model. The model had statistical significance with an F-statistic of 26.33 and 
R^{2} of 0.1097. Our variables of interest were statistically significant at the 
0.05 level with the exception of Car.Miles. In model 2 we included the "Overall 
Quality" variable and our results improved significantly resulting in a R^{2} of
0.6537 and an F-statistic of 335.7 . Lastly, Model 3 included 86 all variables 
in the model,the mechanics behind the OLS regression automatically leads to a 
higher R^{2} (0.94) but the overall strength of the model was not acceptable 
(F-statistic of 40.4). Table X compares the coefficients to all the three models.

#####2.3.2 Model 1.0
<center>
__Model 1.0: Car.Time.min__
</center>
```{r Naive Model, message=FALSE, warning=FALSE}
#OLS Model 1 - very naive considering only the variab
# naive.model <- lm(SalePrice ~ Car.miles + OverallQual + OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model)

print("Model 1.0: Car.Time.min")

naive.model0 <- lm(SalePrice ~ Car.Time.min + OverallQual +OverallCond + GrLivArea, data = d_train_noNAs)
summary(naive.model0)

```

#####2.3.3 Model 1.1
<center>
__Model 1.1: Walk.Time.min__
</center>
```{r}
# naive.model1 <- lm(SalePrice ~ Walk.miles + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model1)

naive.model1.1 <- lm(SalePrice ~ Walk.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model1.1)
```

#####2.3.4 Model 1.2

<center>
__Model 1.2: Bus.Time.min__
</center>
```{r}
naive.model2 <- lm(SalePrice ~ Bus.time.min + OverallQual +OverallCond + GrLivArea  , data = d_train_noNAs)
summary(naive.model2)
```

All our three naive models did not have good results, but when comparing the three models, Model 1.2 performed relatively better, with the highest R-square value (0.7256). We will use Model 1.2. in a more comprehensive model with all the variables in our dataset.

<center>

`Table XX`

| Variable      | Model 1.0 - Car.Time.min | Model 1.1 - Walk.time.min | Model 1.2 - Bus.time.min |
|---------------|--------------------------|---------------------------|--------------------------|
| Estimate      | 3.353                   | 139                     | 335.71                    |
| Std. Error    | 541.1                   | 10220                     | 153.67                    |
| t value       | 0.006                    | 1.945                     | 2.185                    |
| Pr(>abs(t))   | 0.995                   | 0.0521                    | 0.0291                   |
| Adj R Squared | 0.7234                   | 0.7254                    | 0.7256                    |
| F-Statistic   | 702.5                     | 705.9                      | 706.8                     |
</center>

#####2.3.5 Model 2.0

<center>
__Model 2.0: Comprehensive model + Bus.Time.min__
</center>
```{r}
m2.train <- lm(SalePrice ~ Bus.time.min + ., data = d_train_noNAs)

#Show only relevant coefficients
a2Pval <- summary(m2.train)$coefficients[1:2, 1:4]

a2Pval
```
Although we added all variables to Model 2.0, we also lose degrees of freedom and the entire model becomes relatively weak with an F-Statistic of 52.94. Table XX summarizes the results in regards to our variable of interest. Regardless, our variable of interest was not statistically significant nor substantive. However, the model can be improved by including only relevant variables and our variable of interest. We will use Lasso to sort through the 87 variables in our model and include only the best variables into a new model.

<center>

`Table XX`

| Variable      | Model 2.0 - Full model + Bus.time.min |
|---------------|---------------------------------------|
| Estimate      | 269.9592  |
| Std. Error    | 547.9578  |
| t value       | 0.4926642 |
| Pr(>abs(t)    | 0.6223962 |
| Adj R Squared | 0.9417    |
| F-Statistic   | 52.94     |

  
</center>

###2.4 Lasso

The Lasso function will help us select the variables with the highest predictive power on Sales Price. It might also give us a clearer picture that will help us better assess our hypothesis.

Each curve corresponds to a variable. It shows the path of its coefficient against the ℓ1-norm of the whole coefficient vector at as λ varies. The axis above indicates the number of nonzero coefficients at the current λ, which is the effective degrees of freedom (df) for the lasso. Users may also wish to annotate the curves; this can be done by setting label = TRUE in the plot command.


```{r Lasso Cross Validation}
set.seed(12345) # = Seed for replication

#d_train_noNAs <- na.omit(d_train)

x <- model.matrix(SalePrice ~ ., data = d_train_noNAs)[ ,-79]
y <- d_train_noNAs$SalePrice
 
### We then fit a Lasso regression model (alpha = 1)
fit.lasso <- glmnet(x, y, alpha = 1, family = "gaussian")
plot(fit.lasso, xvar = "lambda", label = TRUE)
 
### Now we cross-validate
cv.lasso <- cv.glmnet(x, y)

#cv.lasso$lambda.min 
#cv.lasso$lambda.1se

#coef(cv.lasso, s = "lambda.min")
```
<center>

`Table XX: Lasso coefficients`

The cross-validation plot suggests that the model works best when it has approximately 11 predictors (when using lambda for the most regularized model such that error is within one standard error of the minimum.). Lasso helped to reduce the number of variables by over 87%. We can use cross-validation to extract coefficients that collectively minimize mean squared error.
 
It includes the cross-validation curve (red dotted line), and upper and lower standard deviation curves along the λ sequence (error bars). Two selected λ’s are indicated by the vertical dotted lines (see below). 

```{r}
plot(cv.lasso) 
title(sub = "Graph 3: Cross Validation for Lasso", cex = 1.5)
```
 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(12345) # = Seed for replication

### Extract coefficients corresponding to lambda.min (minimum mean cross-validated error)
myCoefs <- coef(cv.lasso, s = "lambda.1se")

# print(as.matrix(min.coef))

myLasso.Results <- data.frame(
  features = myCoefs@Dimnames[[1]][ which(myCoefs != 0 ) ], #intercept included
  coefs    = myCoefs              [ which(myCoefs != 0 ) ]  #intercept included
)

# myLasso.Results$level.coefs <- cut(myLasso.Results$coefs, 5, labels = c("lowest","low", "med", "high", "highest"))
# 
# table(myLasso.Results$level.coefs)
# 
# summary(myLasso.Results$coefs)
# 
# ggplot(myLasso.Results) + geom_bar(aes(myLasso.Results$level.coefs, fill = myLasso.Results$level.coefs)) + coord_flip()

```



```{r}
pander(myLasso.Results)
#pander(filter(myLasso.Results, level.coefs != "low" ))
```

</center>

###2.5 Model 4 - Improved OLS

Our new model with only 12 variables (instead of all variables) has an adjusted R-squared of __0.89__ and an F-Statistic of __616.1__ - a significant result in comparison to an adjusted R-square of __0.9417__ and an F-statistic of 52.94 (including all 80 variables!). In this adapted model, our variables of interest (Bus.time.min) have a very small coefficient and is not statistically significant. All other variable were statistically significant at the 0.05 level.

Model 4
```{r}
myLasso.Results <- myLasso.Results[-1,]
var.lasso <- myLasso.Results[,1]
var.lasso

m4.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = d_train_noNAs)

summary(m4.train)
```

Model 5
```{r}

pref.data <- d_train_noNAs %>%
  filter(SalePrice >= 250000)
#& d_train_noNAs$SalePrice > 150000

m5.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = pref.data)

summary(m5.train)
```

###2.5 Results

The result from Model 3 provides evidece against the initial hypothesis that there is a positive association between final sales price and distance from ISU. In fact, the model reveals the opposite, on average, for each additional mintute it takes from a neighborhood to ISU campus is associated with __a price drop of approximately $615.7__ in the final sales price. Although the result is statistically significant at the 0.05 level, the result was not substantive. Therefore, we can conclude that distance from the ISU campus is not a statistically important factor when determining the house/apt sales price. So our
client should be feel free to select a house regardless of its distance to ISU campus.


## 3. Predictive Model
In this section, we aim to find the most important variables that affect the Sales Price and build predictive models based on that. We will compare the effectiveness of different predictive models and choose the best model.

### 3.1 Variable Selection

#### 3.1.1 Correlation with SalePrice
```{r}
# Neighborhood
train <- read.csv("New_Data.csv")
train <- na.omit(train)

```

We would like to see What numeric variables correlated with SalePrice.

According to the correlation table, variable 'Overall Quality'(Overall material and finish quality), the 'GrLivArea' (‘Above Grade’ Living Area) and 'GarageCars'(Size of garage in car capacity) are the top 3 variables that highly correlated to SalePrice.
For the rest,it is also clear the multicollinearity is an issue. For example: the correlation between GarageCars and GarageArea is very high (0.89), and both have similar (high) correlations with SalePrice. The other 6 six variables with a correlation higher than 0.5 with SalePrice are: -TotalBsmtSF: Total square feet of basement area -1stFlrSF: First Floor square feet -FullBath: Full bathrooms above grade -TotRmsAbvGrd: Total rooms above grade (does not include bathrooms) -YearBuilt: Original construction date -YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)

```{r correlation2}
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use = "pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
 #select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
```
Graph 4: Correlation Table of Sales Price and all numeric variables

Since there are several paris of varibles are highly correlated to each other, we can simply drop one of them.
Therefore, I chose to drop 'TotalRmsAbvGrd'(vs GrLivArea); 'GarageArea'(vs GarageCars); 'TotalBsmtSF' (vs X1stFkrSF); 'YearRemodAdd'(vs YearBiult)

```{r drop variables and outliers}
dropVars <- c('YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotalRmsAbvGrd')

train <- train[,!(names(train) %in% dropVars)]

train <- train[-c(524, 1299, 2550),]
write_csv(train, 'Final_data.csv')
```

### 3.2 Random Forest

The correlation table above provide a good overview of the most important numeric variables and multicolinerity among those variables. Next, I want to get an overview of the most important variables including the categorical variables.
```{r rf again}
train <- read.csv('Final_data.csv')
set.seed(123)
quick_RF <- randomForest(x = train[, -76], y = train$SalePrice, ntree = 200,importance = TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:20,], aes(x = reorder(Variables, MSE), y = MSE, fill = MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position = "none")
```
Graph 5: Varaible selection by random forest

### 3.3 Predictive Models

We pick variables that increase MSE if variable is randomly permuted greater than 10% (exclude two variables which are X1ndFlrSF and X2ndFlrSF which representing First/Second floor square feet, and are included in GrLivArea)
```{r ols}

train$Mean.price <- cut(train$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )
a <- train%>%
  filter(Mean.price == 'Higher')

train1 <- train[1:1092,]
test1 <- train[1092:1447,]

```

To generate robust results, we  includes all available predictors, an interaction between OverallQual and YearBuilt (because the quality of Older houses may not as good as the new ones) in a new model.

```{r step function}
 # step function 

train1 <- subset(train1, select = -c(Utilities,Street))
test1 <- subset(test1, select = -c(Utilities,Street))
test1 <- test1[!test1$Id == 1004,]
test1 <- test1[!test1$Id == 1299,]
test1 <- test1[!test1$Id == 1001,]
test1 <- test1[!test1$Id == 1188,]
test1 <- test1[!test1$Id == 411,]
test1 <- test1[!test1$Id == 251,]
test1 <- test1[!test1$Id == 596,]
```

#### 3.3.1 Vector Generalized Linear Models
```{r, Vector Generalized Linear Models}
library(VGAM)
# with interaction term
fit_log <- vglm(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea, family=multinomial, data=train1)
# summarize the fit


# make predictions - training set
probabilities <- predict(fit_log, newdata = train1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(train1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(train1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(train1$Mean.price)[3]

# summarize accuracy - training set
tbl_log1 <- table(predictions, train1$Mean.price)
tbl_log1
sum(diag(tbl_log1))/sum(tbl_log1)

# make predictions - testing set
probabilities <- predict(fit_log, newdata = test1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(test1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(test1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(test1$Mean.price)[3]

# summarize accuracy - testing set
tbl_log2 <- table(predictions, test1$Mean.price)
tbl_log2
sum(diag(tbl_log2))/sum(tbl_log2)
```

#### 3.3.2 Linear Discriminant Analysis
```{r lda}
library(MASS)
LDA <- lda(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea,data = train1)

#  make predictions and summarize accuracy - training set
y_hat_LDA <- predict(LDA, newdata = train1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA1 <- table(train1$Mean.price, z_LDA)
tbl_LDA1
sum(diag(tbl_LDA1))/sum(tbl_LDA1)

# make predictions and summarize accuracy - testing set
y_hat_LDA <- predict(LDA, newdata = test1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA <- table(test1$Mean.price, z_LDA)
tbl_LDA
sum(diag(tbl_LDA))/sum(tbl_LDA)
```

#### 3.3.3 Support Vector Machine

```{r svm}
library(e1071)

classifier = svm(formula = Mean.price ~ . ,data = train1[,-74] ,
                 type = 'C-classification',
                 kernel = 'linear')

# make predictions and summarize accuracy - training set
y_pred = predict(classifier, newdata = train1[,-c(74,75)])
cm1 = table(train1[,75], y_pred)
cm1
sum(diag(cm1))/sum(cm1)

# Prmake predictions and summarize accuracy - test set
y_pred = predict(classifier, newdata = test1[,-c(74,75)])
cm = table(test1[,75], y_pred)
cm
sum(diag(cm))/sum(cm)
```

#### 3.3.4 Random Forest Model

```{r rf predict}

RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred1 = predict(RF, newdata = train1[-c(74,75)])
y_pred2 = predict(RF, newdata = test1[-c(74,75)])

# make predictions and summarize accuracy - training set
rf_cm1 = table(train1[,75], y_pred1)
rf_cm1
sum(diag(rf_cm1))/sum(rf_cm1)

# make predictions and summarize accuracy - testing set
rf_cm = table(test1[,75], y_pred2)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
plot(RF)

```

### 3.4 Results
The prediction accuracy of Vector Generalized Linear Model in training set is 0.8937729.
The prediction accuracy of Vector Generalized Linear Model in testing set is 0.8997135.

The prediction accuracy of Linear Discriminant Analysisr Model in training set is 0.8424908.
The prediction accuracy of Linear Discriminant Analysis Model in testing set is 0.8681948.

The prediction accuracy of Support Vector Machine Model in training set is 0.9798535.
The prediction accuracy of Support Vector Machine Model in testing set is 0.9169054.

The prediction accuracy of Random Forest Model in training set is 0.9945055.
The prediction accuracy of Random Forest Model in testing set is 0.8939828.

To conclude, Vector Generalized Linear Model and Linear Discriminant Analysisr Model gave results with fair accuracy and no obvious underfitting were observed. The Support Vector Machine Model gave the best accuracy in testing set (0.917) and has small overfitting problem in training set (0.980). The Random Forest Model gave the best accuracy in training set (0.995) almost 100%, but significant overfitting of training set was observed as the accuracy for testing set is only 0.894. Therefore, we concluded that the Support Vector Machine Model is our best predictive model.

## 4. Conclusion


Next Steps:
1)Time Series Analysis on Pricing
2)Cluster Analysis on Neighborhood

##Annex

###Data Description

| Variable      | Quantitative? | Class  |
|---------------|---------------|--------|
| 1stFlrSF      | Yes           | int    |
| 2ndFlrSF      | Yes           | int    |
| 3SsnPorch     | Yes           | int    |
| BedroomAbvGr  | Yes           | int    |
| BsmtFinSF1    | Yes           | int    |
| BsmtFinSF2    | Yes           | int    |
| BsmtFullBath  | Yes           | int    |
| BsmtHalfBath  | Yes           | int    |
| BsmtUnfSF     | Yes           | int    |
| EnclosedPorch | Yes           | int    |
| Fireplaces    | Yes           | int    |
| FullBath      | Yes           | int    |
| GarageArea    | Yes           | int    |
| GarageCars    | Yes           | int    |
| GarageYrBlt   | Yes           | int    |
| GrLivArea     | Yes           | int    |
| HalfBath      | Yes           | int    |
| KitchenAbvGr  | Yes           | int    |
| LotArea       | Yes           | int    |
| LotFrontage   | Yes           | int    |
| LowQualFinSF  | Yes           | int    |
| MasVnrArea    | Yes           | int    |
| MiscVal       | Yes           | int    |
| MoSold        | Yes           | int    |
| MSSubClass    | Yes           | int    |
| OpenPorchSF   | Yes           | int    |
| OverallCond   | Yes           | int    |
| OverallQual   | Yes           | int    |
| PoolArea      | Yes           | int    |
| ScreenPorch   | Yes           | int    |
| TotalBsmtSF   | Yes           | int    |
| TotRmsAbvGrd  | Yes           | int    |
| WoodDeckSF    | Yes           | int    |
| YearBuilt     | Yes           | int    |
| YearRemodAdd  | Yes           | int    |
| YrSold.       | Yes           | int    |
| Alley         | No            | Factor |
| BldgType      | No            | Factor |
| BsmtCond      | No            | Factor |
| BsmtExposure  | No            | Factor |
| BsmtFinType1  | No            | Factor |
| BsmtFinType2  | No            | Factor |
| BsmtQual      | No            | Factor |
| CentralAir    | No            | Factor |
| Condition1    | No            | Factor |
| Condition2    | No            | Factor |
| Electrical    | No            | Factor |
| ExterCond     | No            | Factor |
| Exterior1st   | No            | Factor |
| Exterior2nd   | No            | Factor |
| ExterQual     | No            | Factor |
| Fence         | No            | Factor |
| FireplaceQu   | No            | Factor |
| Foundation    | No            | Factor |
| Functional    | No            | Factor |
| GarageCond    | No            | Factor |
| GarageFinish  | No            | Factor |
| GarageQual    | No            | Factor |
| GarageType    | No            | Factor |
| Heating       | No            | Factor |
| HeatingQC     | No            | Factor |
| HouseStyle    | No            | Factor |
| KitchenQual   | No            | Factor |
| LandContour   | No            | Factor |
| LandSlope     | No            | Factor |
| LotConfig     | No            | Factor |
| LotShape      | No            | Factor |
| MasVnrType    | No            | Factor |
| MiscFeature   | No            | Factor |
| MSZoning      | No            | Factor |
| Neighborhood  | No            | Factor |
| PavedDrive    | No            | Factor |
| PoolQC        | No            | Factor |
| RoofMatl      | No            | Factor |
| RoofStyle     | No            | Factor |
| SaleCondition | No            | Factor |
| SaleType      | No            | Factor |
| Street        | No            | Factor |
| Utilities.    | No            | Factor |


###Distance Variables

`Table XX: New variables`
  
| Variable      | Description                                                                                  |
|---------------|----------------------------------------------------------------------------------------------|
| Neighborhood  | The name of the neighborhood from the original data.                                         |
| FullName      | Full name of the neighborhood.                                                               |
| Lat/Log       | The approximate latitude and longitude of the reference point for each neighborhood.         |
| Car.miles     | Average distance of suggested routes between the reference point and the ISU campus.         |
| Car.Time.min  | Average journey duration of suggested routes between the reference point and the ISU campus. |
| Walk.miles    | Average distance of suggested routes between the reference point and the ISU campus.         |
| Walk.time.min | Average journey duration of suggested routes between the reference point and the ISU campus. |
| Bus.time.min  | Average journey duration of suggested routes between the reference point and the ISU campus. |
